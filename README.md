# HARDWARE II: Applied Computer Vision & Robotics with AI  
*Applied Computer Vision & Robotics for Architecture and Interactive Systems*

This repository contains all course materials, session notes, and project guidelines for **HARDWARE II**, giving students hands-on experience in **Computer Vision, AI model training, YOLO deployment, OpenCV, ONNX, and ROS2 integration** for robotics applications. Ethical AI, spatial intelligence, and independent project development are emphasized throughout the course.

---

## ðŸ”§ Core Technology Stack
- **Roboflow** â€“ Dataset management and annotation  
- **YOLO** â€“ Real-time object detection framework  
- **OpenCV** â€“ Image processing and geometry operations  
- **Python** â€“ Main development language  
- **ONNX** â€“ Model optimization and deployment  
- **ROS2** â€“ Robot Operating System  
- **Camera Nodes, AI Inference Nodes, Robot/Arduino Control Nodes**

---

## ðŸ“… Sessions Overview

### **SESSION 1 â€” Computer Vision & AI System Foundations**
**Goal:** Establish foundational understanding of computer vision and AI pipeline, including ethical considerations in architecture and public space.  
**Topics Covered:**
- Computer Vision basics, Pixels, FPS, Resolution
- Machine Learning vs Deep Learning, CNNs
- Classification, Detection, Segmentation
- YOLO introduction and real-time detection
- Ethical AI and bias considerations
- Full system pipeline: Camera â†’ Dataset â†’ Training â†’ Inference â†’ Robot/Actuator

---

### **SESSION 2 â€” Dataset Design & Annotation**
**Goal:** Learn how high-quality data drives AI performance.  
**Topics Covered:**
- Dataset concepts, annotation principles, and class design
- Bias, imbalance, and overfitting
- Data augmentation
- Ethical implications in data collection
- Dataset versioning and YOLO export

---

### **SESSION 3 â€” YOLO Training + ROS2 Perception Preview**
**Goal:** Train AI models and understand integration with ROS2 robotics.  
**Topics Covered:**
- Training vs Inference, Epoch, Loss, mAP
- Precision & Recall, detection errors
- ROS2 introduction: Nodes, Topics, Message flow
- YOLO model deployment as standalone script or ROS2 node
- Debugging lighting, occlusion, and failure cases

---

### **SESSION 4 â€” OpenCV, Tracking & Spatial Intelligence**
**Goal:** Convert detections into real-world, measurable spatial intelligence.  
**Topics Covered:**
- Image coordinate systems and bounding box geometry
- Object tracking and ID assignment
- Distance and speed estimation
- Perspective correction and homography for spatial mapping
- Publishing spatial data to screen or ROS2 topic

---

### **SESSION 5 â€” Deployment, ONNX & Full System Integration**
**Goal:** Deploy a complete, autonomous AI system.  
**Topics Covered:**
- Deployment concepts, ONNX, Edge AI vs Cloud AI
- Model size vs speed tradeoff
- Full pipeline: AI â†’ ROS2 â†’ Robot/Actuator
- YOLO to ONNX conversion and inference
- Integration with ROS2 nodes or Arduino/Robot controllers
- System performance metrics (FPS, latency, accuracy)

---

### **Post-Session 5 â€” Independent Project Work**
**Goal:** Apply all knowledge to design and implement a full AI/robotics system.  
**Deliverables:**
- Fully integrated system: YOLO model, ONNX deployment, real-time inference connected to ROS2 or Arduino
- Documentation: system diagram, metrics, design reflections
- Demonstration (live or recorded)
- Optional integration with Grasshopper or Unity for interactive installations

**Final Project Requirements:**
- Real-time detection
- Custom dataset and trained AI model
- Quantitative outputs (distance, speed, count, density)
- Physical or interactive response
- Integration with ROS2 or Arduino

---
